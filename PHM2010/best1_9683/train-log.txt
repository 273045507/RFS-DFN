C:\Users\fangzhaobo\.conda\envs\pytorch\python.exe D:\py\PHM2010\train.py 
C:\Users\fangzhaobo\.conda\envs\pytorch\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Epoch [1], Train Loss: 0.3127, Train Accuracy: 0.8841, Val Loss: 0.1874, Val Accuracy: 0.9175
保存新的最佳模型：best_model11.pth
Epoch [2], Train Loss: 0.1278, Train Accuracy: 0.9476, Val Loss: 0.1212, Val Accuracy: 0.9619
保存新的最佳模型：best_model11.pth
Epoch [3], Train Loss: 0.0633, Train Accuracy: 0.9746, Val Loss: 0.1413, Val Accuracy: 0.9397
Epoch [4], Train Loss: 0.0664, Train Accuracy: 0.9746, Val Loss: 0.1566, Val Accuracy: 0.9524
Epoch [5], Train Loss: 0.0316, Train Accuracy: 0.9841, Val Loss: 0.1586, Val Accuracy: 0.9397
Epoch [6], Train Loss: 0.0205, Train Accuracy: 0.9968, Val Loss: 0.1389, Val Accuracy: 0.9524
Epoch [7], Train Loss: 0.0069, Train Accuracy: 1.0000, Val Loss: 0.1162, Val Accuracy: 0.9683
保存新的最佳模型：best_model11.pth
Epoch [8], Train Loss: 0.0077, Train Accuracy: 0.9984, Val Loss: 0.3919, Val Accuracy: 0.8825
Epoch [9], Train Loss: 0.0365, Train Accuracy: 0.9889, Val Loss: 0.1206, Val Accuracy: 0.9556
Epoch [10], Train Loss: 0.0144, Train Accuracy: 0.9952, Val Loss: 0.1664, Val Accuracy: 0.9365

Process finished with exit code 0
