C:\Users\fangzhaobo\.conda\envs\pytorch\python.exe D:\py\Mudestreda\train_Aug_Att5.py C:\Users\fangzhaobo\.conda\envs\pytorch\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)Epoch [1/5], Train Loss: 0.5673, Train Acc: 0.7637, Val Loss: 0.1752, Val Acc: 0.9808保存新的最佳模型：best_model4.pthEpoch [2/5], Train Loss: 0.2551, Train Acc: 0.8959, Val Loss: 0.1492, Val Acc: 0.9423Epoch [3/5], Train Loss: 0.1180, Train Acc: 0.9595, Val Loss: 0.0145, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [4/5], Train Loss: 0.0604, Train Acc: 0.9813, Val Loss: 0.0077, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [5/5], Train Loss: 0.0260, Train Acc: 0.9948, Val Loss: 0.0046, Val Acc: 1.0000保存新的最佳模型：best_model4.pthTest Loss: 0.0094, Test Acc: 1.0000Process finished with exit code 0