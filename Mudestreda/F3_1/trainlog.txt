C:\Users\fangzhaobo\.conda\envs\pytorch\python.exe D:\py\Mudestreda\train_Aug_Att5.py C:\Users\fangzhaobo\.conda\envs\pytorch\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)Epoch [1/5], Train Loss: 0.5987, Train Acc: 0.7506, Val Loss: 0.1931, Val Acc: 0.9423保存新的最佳模型：best_model4.pthEpoch [2/5], Train Loss: 0.1890, Train Acc: 0.9404, Val Loss: 0.0756, Val Acc: 0.9808保存新的最佳模型：best_model4.pthEpoch [3/5], Train Loss: 0.0951, Train Acc: 0.9670, Val Loss: 0.0430, Val Acc: 0.9808保存新的最佳模型：best_model4.pthEpoch [4/5], Train Loss: 0.0364, Train Acc: 0.9925, Val Loss: 0.0195, Val Acc: 0.9808保存新的最佳模型：best_model4.pthEpoch [5/5], Train Loss: 0.0194, Train Acc: 0.9948, Val Loss: 0.0028, Val Acc: 1.0000保存新的最佳模型：best_model4.pthTest Loss: 0.0209, Test Acc: 1.0000Process finished with exit code 0