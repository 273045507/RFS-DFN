C:\Users\fangzhaobo\.conda\envs\pytorch\python.exe D:\py\Mudestreda\train_Aug_Att5.py C:\Users\fangzhaobo\.conda\envs\pytorch\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)Epoch [1/5], Train Loss: 0.5842, Train Acc: 0.7411, Val Loss: 0.2142, Val Acc: 0.8846保存新的最佳模型：best_model4.pthEpoch [2/5], Train Loss: 0.2623, Train Acc: 0.9011, Val Loss: 0.0304, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [3/5], Train Loss: 0.1399, Train Acc: 0.9496, Val Loss: 0.0277, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [4/5], Train Loss: 0.0717, Train Acc: 0.9766, Val Loss: 0.0020, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [5/5], Train Loss: 0.0471, Train Acc: 0.9849, Val Loss: 0.0024, Val Acc: 1.0000保存新的最佳模型：best_model4.pthTest Loss: 0.0186, Test Acc: 1.0000Process finished with exit code 0