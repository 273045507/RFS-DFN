C:\Users\fangzhaobo\.conda\envs\pytorch\python.exe D:\py\Mudestreda\train_Aug_Att5.py C:\Users\fangzhaobo\.conda\envs\pytorch\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)Epoch [1/5], Train Loss: 0.4578, Train Acc: 0.7998, Val Loss: 0.1439, Val Acc: 0.9808保存新的最佳模型：best_model4.pthEpoch [2/5], Train Loss: 0.1535, Train Acc: 0.9496, Val Loss: 0.0266, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [3/5], Train Loss: 0.0561, Train Acc: 0.9857, Val Loss: 0.0207, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [4/5], Train Loss: 0.0245, Train Acc: 0.9940, Val Loss: 0.0060, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [5/5], Train Loss: 0.0133, Train Acc: 0.9964, Val Loss: 0.0017, Val Acc: 1.0000保存新的最佳模型：best_model4.pthTest Loss: 0.0173, Test Acc: 1.0000Process finished with exit code 0