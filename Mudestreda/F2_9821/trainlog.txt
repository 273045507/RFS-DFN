C:\Users\fangzhaobo\.conda\envs\pytorch\python.exe D:\py\Mudestreda\train_Aug_Att5.py C:\Users\fangzhaobo\.conda\envs\pytorch\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)Epoch [1/5], Train Loss: 0.5203, Train Acc: 0.7724, Val Loss: 0.2151, Val Acc: 0.9038保存新的最佳模型：best_model4.pthEpoch [2/5], Train Loss: 0.1666, Train Acc: 0.9440, Val Loss: 0.0578, Val Acc: 0.9808保存新的最佳模型：best_model4.pthEpoch [3/5], Train Loss: 0.0603, Train Acc: 0.9837, Val Loss: 0.0139, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [4/5], Train Loss: 0.0287, Train Acc: 0.9921, Val Loss: 0.0047, Val Acc: 1.0000保存新的最佳模型：best_model4.pthEpoch [5/5], Train Loss: 0.0215, Train Acc: 0.9936, Val Loss: 0.0042, Val Acc: 1.0000保存新的最佳模型：best_model4.pthTest Loss: 0.0235, Test Acc: 0.9821Process finished with exit code 0